# ViT-from-scratch
Implementing a Vision Transformer (ViT) for Image Classification

data from: https://www.kaggle.com/datasets/fitnesschum/arab-fari-mnist

This project focuses on implementing a Vision Transformer (ViT) from scratch for image classification tasks. Unlike traditional convolutional neural networks (CNNs), ViT leverages self-attention mechanisms to capture spatial dependencies across the entire image, enabling improved performance on various vision tasks.

The implementation will include:

- Data preprocessing and patch embedding
- Multi-head self-attention mechanisms
- Transformer encoder layers
- Positional encoding
- Classification head for final predictions
- The model will be trained and evaluated on standard benchmark datasets such as CIFAR-10 or ImageNet to compare its performance with CNN-based architectures. The project aims to provide insights into the workings of ViT and its effectiveness in image classification.
